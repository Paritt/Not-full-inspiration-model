# -*- coding: utf-8 -*-
"""1180_Lung_segmentation_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qIqeQkdNvUp3krsyMwQ-47VDe1FHKjO_

#**Lung Segmentation**
By **segmentation_model** framework

#Check GPU
"""

!nvidia-smi

"""#Import package"""

pip install -U segmentation-models

import tensorflow as tf
import segmentation_models as sm
import glob
import cv2
import os
import numpy as np
from matplotlib import pyplot as plt
import keras 
import datetime

from sklearn.preprocessing import LabelEncoder
from keras.utils.all_utils import normalize 
from keras.metrics import MeanIoU
sm.set_framework('tf.keras')
sm.framework()

"""#Lung path"""

from google.colab import drive #เชื่อมggcolabกับdriveเรา
drive.mount('/content/drive')

images_path = '/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/NEW_MODEL_UNET/TRAIN_DATA_LUNG/Train_Aug/Image'
masks_path = '/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/NEW_MODEL_UNET/TRAIN_DATA_LUNG/Train_Aug/Mask'

val_images_path = '/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/NEW_MODEL_UNET/TRAIN_DATA_LUNG/Validation/Image'
val_mask_path ='/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/NEW_MODEL_UNET/TRAIN_DATA_LUNG/Validation/Mask'

test_images_path = '/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/NEW_MODEL_UNET/IMAGE_TEST_200'
test_mask_path = '/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/NEW_MODEL_UNET/MASK_GT_TEST/ไม่แยกFULL_NOTFULL/mask by myu/Lung'

"""#Helper function


"""

def load_image(images_path):
  train_images_path = []
  train_images = []

  for directory_path in glob.glob(images_path):
      for img_path in glob.glob(os.path.join(directory_path, "*.jpeg")):
          train_images_path.append(img_path)

  train_images_path.sort() 

  for img_path in train_images_path:
          img = cv2.imread(img_path, 1) #Read in BGR mode (1)      
          #img = cv2.resize(img, (SIZE_Y, SIZE_X))
          #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
          #img = img/255 
          train_images.append(img)

  train_images = np.array(train_images) #Change it to numpy array

  return train_images, train_images_path

def load_mask(masks_path):
  train_masks_path = [] 
  train_masks = [] 

  for directory_path in glob.glob(masks_path):
      for mask_path in glob.glob(os.path.join(directory_path, "*.tiff")):
          train_masks_path.append(mask_path)

  train_masks_path.sort()

  for mask_path in train_masks_path:
          mask = cv2.imread(mask_path, 0) #Read in grayscale mode (0)       
          mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation
          train_masks.append(mask)

  train_masks = np.array(train_masks) #Change it to numpy array

  return train_masks, train_masks_path


def load_image_test(images_path):
  train_images_path = []
  train_images = []

  for directory_path in glob.glob(images_path):
      for img_path in glob.glob(os.path.join(directory_path, "*.png")):
          train_images_path.append(img_path)

  train_images_path.sort() 

  for img_path in train_images_path:
          img = cv2.imread(img_path, 1) #Read in BGR mode (1)      
          img = cv2.resize(img, (SIZE_Y, SIZE_X))
          #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
          #img = img/255 
          train_images.append(img)

  train_images = np.array(train_images) #Change it to numpy array

  return train_images, train_images_path

def load_mask_test(masks_path):
  train_masks_path = [] 
  train_masks = [] 

  for directory_path in glob.glob(masks_path):
      for mask_path in glob.glob(os.path.join(directory_path, "*.tiff")):
          train_masks_path.append(mask_path)

  train_masks_path.sort()

  for mask_path in train_masks_path:
          mask = cv2.imread(mask_path, 0) #Read in grayscale mode (0)       
          mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation
          train_masks.append(mask)

  train_masks = np.array(train_masks) #Change it to numpy array

  return train_masks, train_masks_path

def labelencoder(train_masks):
  labelencoder = LabelEncoder()
  n, h, w = train_masks.shape
  train_masks_reshaped = train_masks.reshape(-1,1)
  train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)
  train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)
  return train_masks_encoded_original_shape
  
def see_image_mask_predict(images_batch,mask_batch,model,img_num=0):
    img = images_batch[img_num]
    ground_truth = mask_batch[img_num]

    img_input = np.expand_dims(img, 0)
    img_input = preprocess_input(img_input)

    y_pred = model(img_input)
    y_pred_argmax = np.argmax(y_pred, axis=3)[0,:,:]

    plt.figure(figsize=(12, 8))
    plt.subplot(231)
    plt.title('Image')
    plt.imshow(img)
    plt.axis('off')
    plt.subplot(232)
    plt.title('Mask')
    plt.imshow(ground_truth[:,:,0])
    plt.axis('off')
    plt.subplot(233)
    plt.title('Prediction')
    plt.imshow(y_pred_argmax)
    plt.axis('off')

def learning_curve(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  epochs = range(1, len(loss) + 1)
  plt.plot(epochs, loss, 'b', label='Training loss')
  plt.plot(epochs, val_loss, 'r', label='Validation loss')
  plt.title('Training and validation loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.show()

  acc = history.history['iou_score']
  val_acc = history.history['val_iou_score']

  plt.plot(epochs, acc, 'b', label='Training IOU')
  plt.plot(epochs, val_acc, 'r', label='Validation IOU')
  plt.title('Training and validation IOU')
  plt.xlabel('Epochs')
  plt.ylabel('IOU')
  plt.legend()
  plt.show()

def predict_on_image(img_path,model,BACKBONE):
  preprocess_input = sm.get_preprocessing(BACKBONE)
  img = cv2.imread(img_path, 1) #Read in BGR mode (1)      
  img = cv2.resize(img, (SIZE_Y, SIZE_X))
  img_input = np.expand_dims(img, 0)
  img_input = preprocess_input(img_input)
  y_pred = model.predict(img_input)
  y_pred_argmax = np.argmax(y_pred, axis=3)[0,:,:]
  plt.figure(figsize=(12, 8))
  plt.subplot(121)
  plt.title('Image')
  plt.imshow(img)
  plt.axis('off')
  plt.subplot(122)
  plt.title('Prediction')
  plt.imshow(y_pred_argmax)
  plt.axis('off')

"""#Prepare Dataset"""

SIZE_X = 512 
SIZE_Y = 512
n_classes = 3

"""##Load dataset

**Train**
"""

X_train, train_images_path = load_image(images_path=images_path)
y_train, train_masks_path = load_mask(masks_path=masks_path)

"""**Validation**"""

X_val, val_img_path = load_image_test(images_path=val_images_path)
y_val, val_mask_path = load_mask_test(masks_path=val_mask_path)

"""**Test**"""

X_test, test_img_path = load_image_test(images_path=test_images_path)
y_test, test_mask_path = load_mask_test(masks_path=test_mask_path)

print('Image')
print(f'Train: {X_train.shape}')
print(f'Val: {X_val.shape}')
print(f'Test: {X_test.shape}')
print('\n')
print('Mask')
print(f'Train: {y_train.shape}')
print(f'Val: {y_val.shape}')
print(f'Test: {y_test.shape}')

"""##Label Encoder"""

y_train = labelencoder(y_train)
y_val = labelencoder(y_val)
y_test = labelencoder(y_test)

"""##Expand dimension"""

y_train = np.expand_dims(y_train, axis=3)
y_val = np.expand_dims(y_val, axis=3)
y_test = np.expand_dims(y_test, axis=3)

np.unique(y_train)

for n in range(len(y_train)):
  print(f'Image {n}: {np.unique(y_train[n][:,:,0])}')

def load_mask_2(masks_path):
  train_masks_path = [] 
  train_masks = [] 

  for directory_path in glob.glob(masks_path):
      for mask_path in glob.glob(os.path.join(directory_path, "*.tiff")):
          train_masks_path.append(mask_path)

  train_masks_path.sort()
  for i in range(len(train_masks_path)):
    print(train_masks_path[i])

load_mask_2('/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/RIB_UNET_2/400train/Train/mask')

np.unique(y_val)

np.unique(y_test)

"""##One hot encode"""

from keras.utils.all_utils import to_categorical

train_masks_cat = to_categorical(y_train, num_classes = n_classes)
y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))

validation_masks_cat = to_categorical(y_val, num_classes=n_classes)
y_val_cat = validation_masks_cat.reshape((y_val.shape[0], y_val.shape[1], y_val.shape[2], n_classes))

test_masks_cat = to_categorical(y_test, num_classes=n_classes)
y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))

"""##Summary"""

print(f'Train dataset image shape: {X_train.shape}')
print(f'Train dataset mask shape: {y_train_cat.shape}')
print(f"Class values in the train dataset are {np.unique(y_train_cat)}\n ")

print(f'Validation dataset image shape: {X_val.shape}') 
print(f'Validation dataset mask shape: {y_val_cat.shape}')
print(f"Class values in the validation dataset are {np.unique(y_val_cat)}\n ")

print(f'Test dataset image shape: {X_test.shape}') 
print(f'Test dataset mask shape: {y_test_cat.shape}')
print(f"Class values in the test dataset are {np.unique(y_test_cat)} \n")

"""#Create Model

**Hyperparameter**
"""

activation='softmax'
BATCH_SIZE = 8
EPOCHS = 100
LR = 0.0001
optim = tf.keras.optimizers.Adam(LR)

# Segmentation models losses can be combined together by '+' and scaled by integer or float factor
dice_loss = sm.losses.DiceLoss(class_weights=np.array([0.25, 0.25, 0.25])) 
focal_loss = sm.losses.CategoricalFocalLoss()
total_loss = dice_loss + (1 * focal_loss)

Jaccard_loss = sm.losses.JaccardLoss()

# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses
# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss 

metrics = [sm.metrics.IOUScore(threshold=0.5)]

"""**Back Bone**

'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'seresnet18', 'seresnet34', 'seresnet50', 'seresnet101', 'seresnet152', 'seresnext50', 'seresnext101', 'senet154', 'resnext50', 'resnext101', 'vgg16', 'vgg19', 'densenet121', 'densenet169', 'densenet201', 'inceptionresnetv2', 'inceptionv3', 'mobilenet', 'mobilenetv2', 'efficientnetb0', 'efficientnetb1', 'efficientnetb2', 'efficientnetb3', 'efficientnetb4', 'efficientnetb5', 'efficientnetb6', 'efficientnetb7'
"""

BACKBONE = 'resnet50'
preprocess_input = sm.get_preprocessing(BACKBONE)

# preprocess input
X_train_p = preprocess_input(X_train)
X_val_p = preprocess_input(X_val)
X_test_p = preprocess_input(X_test)

# define model
model = sm.Unet(BACKBONE, 
                encoder_weights='imagenet', 
                classes=n_classes, 
                activation=activation,
                encoder_freeze=False
                )

# compile keras model with defined optimozer, loss and metrics
model.compile(optim, Jaccard_loss, metrics=metrics)

"""#Train"""

start = datetime.datetime.now()

history = model.fit(X_train_p,  
                    y_train_cat,
                    batch_size=BATCH_SIZE, 
                    epochs=EPOCHS,
                    verbose=1,
                    validation_data=(X_val_p, y_val_cat))

end = datetime.datetime.now()
print(f"Training time: {end - start}")

"""##Learning curve"""

learning_curve(history)

model.save('/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/NEW_MODEL_UNET/SAVED_MODEL/lungmodel_resnet50_1180_e100_b4_lr0.0001.hdf5')

"""#Evaluate

##Load model
"""

model = tf.keras.models.load_model('/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/NEW_MODEL_UNET/SAVED_MODEL/lungmodel_resnet50_1180_e100_b4_lr0.0001.hdf5', compile=False)

BACKBONE = 'resnet50'
preprocess_input = sm.get_preprocessing(BACKBONE)

"""##Mean IoU"""

y_pred = model.predict(X_train_p)
y_pred_argmax = np.argmax(y_pred, axis=3)
IOU_keras = MeanIoU(num_classes=n_classes)  
IOU_keras.update_state(y_train[:,:,:,0], y_pred_argmax)
print("Mean IoU on train =", IOU_keras.result().numpy())

y_pred = model.predict(X_test_p)
y_pred_argmax = np.argmax(y_pred, axis=3)
IOU_keras = MeanIoU(num_classes=n_classes)  
IOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)
print("Mean IoU on test =", IOU_keras.result().numpy())

"""##IoU for class"""

#To calculate I0U for each class...
values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)
#print(values)
class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[1,0]+ values[2,0])
class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[0,1]+ values[2,1])
class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[0,2]+ values[1,2])

print("IoU for class1 is: ", class1_IoU)
print("IoU for class2 is: ", class2_IoU)
print("IoU for class3 is: ", class3_IoU)

"""##Prediction on train"""

#for i in range(len(X_train_p)):
for i in range(5):
  see_image_mask_predict(images_batch = X_train,
                        mask_batch = y_train,
                        model = model,
                        img_num = i)

"""##Prediction on test"""

for i in range(len(X_test_p)):
  see_image_mask_predict(images_batch = X_test,
                          mask_batch = y_test,
                          model = model,
                          img_num = i)

"""##Prediction on image"""

img_path = '/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/NEW_MODEL_UNET/IMAGE_TEST_200/140.png'

predict_on_image(img_path,model, BACKBONE='resnet50')

img_path = '/content/drive/Shareddrives/CXR_Project/Code/Colab_Notebook/Full Inspired/NEW_MODEL_UNET/IMAGE_TEST_200/004.png'

predict_on_image(img_path,model, BACKBONE='resnet50')